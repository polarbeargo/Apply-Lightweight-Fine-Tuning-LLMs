{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: \n",
    "* Model: \n",
    "* Evaluation approach: \n",
    "* Fine-tuning dataset: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7def5b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from peft import LoraConfig, get_peft_model, AutoPeftModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2f31f6",
   "metadata": {},
   "source": [
    "- Prepare a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f551c63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"ccdv/arxiv-classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a738d1",
   "metadata": {},
   "source": [
    "- Tokenizer to process the text include a padding and truncation strategy to handle any variable sequence lengths. Using Hugging Face's datasets map method to apply a preprocessing function over the entire dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4935cb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 28388/28388 [06:26<00:00, 73.54 examples/s]\n",
      "Map: 100%|██████████| 2500/2500 [00:35<00:00, 70.69 examples/s]\n",
      "Map: 100%|██████████| 2500/2500 [00:33<00:00, 74.35 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa99566",
   "metadata": {},
   "source": [
    "- Create a smaller subset of the full dataset to fine-tune on to reduce the time it takes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f28c4a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29413ed6",
   "metadata": {},
   "source": [
    "- Loading model and specify the number of expected labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "019b9f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|██████████| 436M/436M [00:07<00:00, 57.9MB/s] \n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2aa600d",
   "metadata": {},
   "source": [
    "- Create a TrainingArguments class which contains all the hyperparameters we can tune as well as flags for activating different training options then specify where to save the checkpoints from training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5176b07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cc68f4",
   "metadata": {},
   "source": [
    "- Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "edf88813",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05f1b884",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bd9e1c",
   "metadata": {},
   "source": [
    "- Create a Trainer object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2724c0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80137c1f",
   "metadata": {},
   "source": [
    "- Fine-tune your model by calling train():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6684266c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [04:41<00:00,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 281.7849, 'train_samples_per_second': 10.646, 'train_steps_per_second': 1.331, 'train_loss': 1.083978759765625, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=375, training_loss=1.083978759765625, metrics={'train_runtime': 281.7849, 'train_samples_per_second': 10.646, 'train_steps_per_second': 1.331, 'train_loss': 1.083978759765625, 'epoch': 3.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdc4deb",
   "metadata": {},
   "source": [
    "- Creating a PEFT Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5775fadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LoraConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca22a5fd",
   "metadata": {},
   "source": [
    "- Converting a Transformers Model into a PEFT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "894046c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 665/665 [00:00<00:00, 44.6kB/s]\n",
      "model.safetensors: 100%|██████████| 548M/548M [00:07<00:00, 69.5MB/s] \n",
      "generation_config.json: 100%|██████████| 124/124 [00:00<00:00, 122kB/s]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b07d70a",
   "metadata": {},
   "source": [
    "- Training with a PEFT Model And Checking Trainable Parameters of a PEFT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c4d4c908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 294,912 || all params: 124,734,720 || trainable%: 0.23643136409814364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hsin-wenchang/Documents/GitHub/Apply-Lightweight-Fine-Tuning-to-a-Foundation-Model/.env/lib/python3.9/site-packages/peft/tuners/lora/layer.py:711: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "lora_model = get_peft_model(model, config)\n",
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478eaa2c",
   "metadata": {},
   "source": [
    "- Saving a Trained PEFT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b47abf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model.save_pretrained(\"gpt-lora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22de9a84",
   "metadata": {},
   "source": [
    "- Loading a Saved PEFT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model = AutoPeftModelForCausalLM.from_pretrained(\"gpt-lora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebf388a",
   "metadata": {},
   "source": [
    "- Generating Text from a PEFT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bc3a8147",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vocab.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 12.2MB/s]\n",
      "merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 18.7MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 59.1MB/s]\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello, my name is _____. I am a student at the University of']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "inputs = tokenizer(\"Hello, my name is \", return_tensors=\"pt\")\n",
    "outputs = model.generate(input_ids=inputs[\"input_ids\"], max_new_tokens=10)\n",
    "print(tokenizer.batch_decode(outputs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

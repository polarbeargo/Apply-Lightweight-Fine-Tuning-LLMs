{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Lightweight Fine-Tuning Project","metadata":{}},{"cell_type":"markdown","source":"TODO: In this cell, describe your choices for each of the following\n\n* PEFT technique: \n* Model: \n* Evaluation approach: \n* Fine-tuning dataset: ","metadata":{}},{"cell_type":"markdown","source":"## Loading and Evaluating a Foundation Model\n\nTODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset.","metadata":{}},{"cell_type":"code","source":"%pip -q install git+https://github.com/huggingface/transformers.git\n%pip install -q -U git+https://github.com/huggingface/trl\n%pip install -U accelerate\n%pip install wandb\n%pip install torch torchvision\n%pip install peft\n%pip install numpy\n%pip install datasets\n%pip install evaluate\n%pip install sklearn\n%pip install matplotlib\n%pip install ipywidgets\n%pip install bitsandbytes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%pip install peft\n%pip install -U accelerate\n%pip install -U trl \n%pip install datasets==2.16.0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%pip install bitsandbytes\n!pip install GPUtil","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, TrainingArguments, Trainer, BitsAndBytesConfig, AutoModelForCausalLM, default_data_collator\nfrom datasets import load_dataset\nimport numpy as np\nfrom enum import Enum\n# import evaluate\nfrom peft import LoraConfig, get_peft_model, PeftModel, prepare_model_for_kbit_training\nfrom torch.utils.data import Dataset, DataLoader\nimport os, wandb, torch\nfrom trl import SFTTrainer","metadata":{"execution":{"iopub.status.busy":"2024-02-29T22:59:53.284787Z","iopub.execute_input":"2024-02-29T22:59:53.285075Z","iopub.status.idle":"2024-02-29T23:00:01.616192Z","shell.execute_reply.started":"2024-02-29T22:59:53.285030Z","shell.execute_reply":"2024-02-29T23:00:01.615147Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-02-29 22:59:58.013481: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-29 22:59:58.013538: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-29 22:59:58.015100: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_hf = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\nsecret_wandb = user_secrets.get_secret(\"wandb\")","metadata":{"execution":{"iopub.status.busy":"2024-02-29T23:00:05.546574Z","iopub.execute_input":"2024-02-29T23:00:05.546960Z","iopub.status.idle":"2024-02-29T23:00:05.966156Z","shell.execute_reply.started":"2024-02-29T23:00:05.546929Z","shell.execute_reply":"2024-02-29T23:00:05.965131Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"!huggingface-cli login --token $secret_hf","metadata":{"execution":{"iopub.status.busy":"2024-02-29T23:00:08.409509Z","iopub.execute_input":"2024-02-29T23:00:08.410150Z","iopub.status.idle":"2024-02-29T23:00:10.139273Z","shell.execute_reply.started":"2024-02-29T23:00:08.410114Z","shell.execute_reply":"2024-02-29T23:00:10.138125Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"wandb.login(key = secret_wandb)\nrun = wandb.init(\n    project='Fine tuning gpt2', \n    job_type=\"training\", \n    anonymous=\"allow\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-02-29T23:09:33.886876Z","iopub.execute_input":"2024-02-29T23:09:33.887801Z","iopub.status.idle":"2024-02-29T23:10:04.507805Z","shell.execute_reply.started":"2024-02-29T23:09:33.887766Z","shell.execute_reply":"2024-02-29T23:10:04.506897Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240229_230933-8e0aigfy</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/bow1226/Fine%20tuning%20gpt2/runs/8e0aigfy' target=\"_blank\">eager-night-1</a></strong> to <a href='https://wandb.ai/bow1226/Fine%20tuning%20gpt2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/bow1226/Fine%20tuning%20gpt2' target=\"_blank\">https://wandb.ai/bow1226/Fine%20tuning%20gpt2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/bow1226/Fine%20tuning%20gpt2/runs/8e0aigfy' target=\"_blank\">https://wandb.ai/bow1226/Fine%20tuning%20gpt2/runs/8e0aigfy</a>"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nfrom GPUtil import showUtilization as gpu_usage\nfrom numba import cuda\n\ndef free_gpu_cache():\n    print(\"Initial GPU Usage\")\n    gpu_usage()                             \n\n    torch.cuda.empty_cache()\n\n    cuda.select_device(0)\n    cuda.close()\n    cuda.select_device(0)\n\n    print(\"GPU Usage after emptying the cache\")\n    gpu_usage()\n\nfree_gpu_cache()  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Prepare a dataset","metadata":{}},{"cell_type":"code","source":"base_model = \"google/gemma-2b\"\ndataset_name = \"mlabonne/guanaco-llama2-1k\"\nnew_model = \"google/gemma-2b\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_model = \"gpt2\"\ndataset_name = \"mlabonne/guanaco-llama2-1k\"\nnew_model = \"gpt2\"","metadata":{"execution":{"iopub.status.busy":"2024-02-29T23:10:17.292771Z","iopub.execute_input":"2024-02-29T23:10:17.293765Z","iopub.status.idle":"2024-02-29T23:10:17.299427Z","shell.execute_reply.started":"2024-02-29T23:10:17.293722Z","shell.execute_reply":"2024-02-29T23:10:17.298278Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(\n        base_model,\n        device_map=\"auto\",\n        trust_remote_code=True,\n)\nmodel.config.use_cache = False # silence the warnings\nmodel.config.pretraining_tp = 1\nmodel.gradient_checkpointing_enable()","metadata":{"execution":{"iopub.status.busy":"2024-02-29T23:10:21.745972Z","iopub.execute_input":"2024-02-29T23:10:21.746397Z","iopub.status.idle":"2024-02-29T23:10:22.731579Z","shell.execute_reply.started":"2024-02-29T23:10:21.746366Z","shell.execute_reply":"2024-02-29T23:10:22.730559Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\n# model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\", device_map=\"auto\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\ntokenizer.padding_side = 'right'\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.add_eos_token = True\ntokenizer.add_bos_token, tokenizer.add_eos_token","metadata":{"execution":{"iopub.status.busy":"2024-02-29T23:10:29.098580Z","iopub.execute_input":"2024-02-29T23:10:29.098977Z","iopub.status.idle":"2024-02-29T23:10:29.338947Z","shell.execute_reply.started":"2024-02-29T23:10:29.098947Z","shell.execute_reply":"2024-02-29T23:10:29.337762Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"(False, True)"},"metadata":{}}]},{"cell_type":"code","source":"#Importing the dataset\ndataset = load_dataset(dataset_name, split=\"train\")\ndataset[\"text\"][100]","metadata":{"execution":{"iopub.status.busy":"2024-02-29T23:10:34.112650Z","iopub.execute_input":"2024-02-29T23:10:34.113540Z","iopub.status.idle":"2024-02-29T23:10:36.022751Z","shell.execute_reply.started":"2024-02-29T23:10:34.113504Z","shell.execute_reply":"2024-02-29T23:10:36.021737Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"'<s>[INST] cuanto es 2x2 xD [/INST] La respuesta es 4. </s><s>[INST] puedes demostrarme matematicamente que 2x2 es 4? [/INST] En una multiplicación, el producto es el resultado de sumar un factor tantas veces como indique el otro, es decir, si tenemos una operación v · n = x, entonces x será igual a v sumado n veces o n sumado v veces, por ejemplo, para la multiplicación 3 · 4 podemos sumar \"3 + 3 + 3 + 3\" o \"4 + 4 + 4\" y en ambos casos nos daría como resultado 12, para el caso de 2 · 2 al ser iguales los dos factores el producto sería \"2 + 2\" que es igual a 4 </s>'"},"metadata":{}}]},{"cell_type":"code","source":"# torch.backends.cuda.enable_mem_efficient_sdp(False)\n# torch.backends.cuda.enable_flash_sdp(False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Create a TrainingArguments class which contains all the hyperparameters we can tune as well as flags for activating different training options then specify where to save the checkpoints from training","metadata":{}},{"cell_type":"code","source":"training_arguments = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=1,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=1,\n    optim=\"paged_adamw_32bit\",\n    save_steps=25,\n    logging_steps=25,\n    learning_rate=2e-4,\n    weight_decay=0.001,\n    fp16=False,\n    bf16=False,\n    max_grad_norm=0.3,\n    max_steps=-1,\n    warmup_ratio=0.03,\n    group_by_length=True,\n    lr_scheduler_type=\"constant\",\n    report_to=\"wandb\"\n)\n\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    max_seq_length= None,\n    dataset_text_field=\"text\",\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing= False,\n)\n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-02-29T23:10:40.058862Z","iopub.execute_input":"2024-02-29T23:10:40.059722Z","iopub.status.idle":"2024-02-29T23:14:22.579408Z","shell.execute_reply.started":"2024-02-29T23:10:40.059688Z","shell.execute_reply":"2024-02-29T23:14:22.578439Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:225: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [250/250 03:39, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>25</td>\n      <td>3.129600</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>3.399700</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>3.075200</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>3.255700</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>2.927400</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>3.094800</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>2.850100</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>3.118600</td>\n    </tr>\n    <tr>\n      <td>225</td>\n      <td>2.850800</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>3.250100</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Checkpoint destination directory ./results/checkpoint-25 already exists and is non-empty. Saving will proceed but saved results may be invalid.\nCheckpoint destination directory ./results/checkpoint-50 already exists and is non-empty. Saving will proceed but saved results may be invalid.\nCheckpoint destination directory ./results/checkpoint-75 already exists and is non-empty. Saving will proceed but saved results may be invalid.\nCheckpoint destination directory ./results/checkpoint-100 already exists and is non-empty. Saving will proceed but saved results may be invalid.\nCheckpoint destination directory ./results/checkpoint-125 already exists and is non-empty. Saving will proceed but saved results may be invalid.\nCheckpoint destination directory ./results/checkpoint-150 already exists and is non-empty. Saving will proceed but saved results may be invalid.\nCheckpoint destination directory ./results/checkpoint-175 already exists and is non-empty. Saving will proceed but saved results may be invalid.\nCheckpoint destination directory ./results/checkpoint-200 already exists and is non-empty. Saving will proceed but saved results may be invalid.\nCheckpoint destination directory ./results/checkpoint-225 already exists and is non-empty. Saving will proceed but saved results may be invalid.\nCheckpoint destination directory ./results/checkpoint-250 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n","output_type":"stream"},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=250, training_loss=3.0952061157226565, metrics={'train_runtime': 221.4367, 'train_samples_per_second': 4.516, 'train_steps_per_second': 1.129, 'total_flos': 243597662208000.0, 'train_loss': 3.0952061157226565, 'epoch': 1.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"- Evaluate","metadata":{}},{"cell_type":"code","source":"wandb.finish()\nmodel.config.use_cache = True\nmodel.eval()","metadata":{"execution":{"iopub.status.busy":"2024-02-29T23:14:27.152210Z","iopub.execute_input":"2024-02-29T23:14:27.152596Z","iopub.status.idle":"2024-02-29T23:14:30.121304Z","shell.execute_reply.started":"2024-02-29T23:14:27.152565Z","shell.execute_reply":"2024-02-29T23:14:30.120444Z"},"trusted":true},"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.058 MB of 0.058 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▂▃▃▄▅▆▆▇██</td></tr><tr><td>train/global_step</td><td>▁▂▃▃▄▅▆▆▇██</td></tr><tr><td>train/grad_norm</td><td>▃█▂▄▁▃▁▄▁▃</td></tr><tr><td>train/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>▅█▄▆▂▄▁▄▁▆</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>250</td></tr><tr><td>train/grad_norm</td><td>4.2398</td></tr><tr><td>train/learning_rate</td><td>0.0002</td></tr><tr><td>train/loss</td><td>3.2501</td></tr><tr><td>train/total_flos</td><td>243597662208000.0</td></tr><tr><td>train/train_loss</td><td>3.09521</td></tr><tr><td>train/train_runtime</td><td>221.4367</td></tr><tr><td>train/train_samples_per_second</td><td>4.516</td></tr><tr><td>train/train_steps_per_second</td><td>1.129</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">eager-night-1</strong> at: <a href='https://wandb.ai/bow1226/Fine%20tuning%20gpt2/runs/8e0aigfy' target=\"_blank\">https://wandb.ai/bow1226/Fine%20tuning%20gpt2/runs/8e0aigfy</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240229_230933-8e0aigfy/logs</code>"},"metadata":{}},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"GPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50257, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-11): 12 x GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"## Performing Parameter-Efficient Fine-Tuning\n\nTODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights.","metadata":{}},{"cell_type":"markdown","source":"- Creating a PEFT Config","metadata":{}},{"cell_type":"code","source":"model = prepare_model_for_kbit_training(model)\npeft_config = LoraConfig(\n    lora_alpha=16,\n    lora_dropout=0.1,\n    r=64,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\nmodel = get_peft_model(model, peft_config)\nprint(model.print_trainable_parameters())\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2024-02-29T23:14:48.946526Z","iopub.execute_input":"2024-02-29T23:14:48.947257Z","iopub.status.idle":"2024-02-29T23:14:49.007805Z","shell.execute_reply.started":"2024-02-29T23:14:48.947225Z","shell.execute_reply":"2024-02-29T23:14:49.006700Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"trainable params: 2,359,296 || all params: 126,799,104 || trainable%: 1.8606566809809635\nNone\nPeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): GPT2LMHeadModel(\n      (transformer): GPT2Model(\n        (wte): Embedding(50257, 768)\n        (wpe): Embedding(1024, 768)\n        (drop): Dropout(p=0.1, inplace=False)\n        (h): ModuleList(\n          (0-11): 12 x GPT2Block(\n            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (attn): GPT2Attention(\n              (c_attn): lora.Linear(\n                (base_layer): Conv1D()\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=768, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=64, out_features=2304, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (c_proj): Conv1D()\n              (attn_dropout): Dropout(p=0.1, inplace=False)\n              (resid_dropout): Dropout(p=0.1, inplace=False)\n            )\n            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (mlp): GPT2MLP(\n              (c_fc): Conv1D()\n              (c_proj): Conv1D()\n              (act): NewGELUActivation()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      )\n      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n    )\n  )\n)\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/tuners/lora/layer.py:861: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"- Using the bitsandbytes package to combine quantization and LoRA. This is also known as QLoRA","metadata":{}},{"cell_type":"code","source":"bnb_config = BitsAndBytesConfig(  \n    load_in_4bit= True,\n    bnb_4bit_quant_type= \"nf4\",\n    bnb_4bit_compute_dtype= torch.bfloat16,\n    bnb_4bit_use_double_quant= False,\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n        base_model,\n        load_in_4bit=True,\n        quantization_config=bnb_config,\n        torch_dtype=torch.bfloat16,\n        device_map=\"auto\",\n        trust_remote_code=True,\n)\nmodel.config.use_cache = False # silence the warnings. Please re-enable for inference!\nmodel.config.pretraining_tp = 1\nmodel.gradient_checkpointing_enable()","metadata":{"execution":{"iopub.status.busy":"2024-02-29T23:14:54.629772Z","iopub.execute_input":"2024-02-29T23:14:54.630171Z","iopub.status.idle":"2024-02-29T23:14:54.955873Z","shell.execute_reply.started":"2024-02-29T23:14:54.630139Z","shell.execute_reply":"2024-02-29T23:14:54.954618Z"},"trusted":true},"execution_count":25,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[25], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m bnb_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(  \n\u001b[1;32m      2\u001b[0m     load_in_4bit\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      3\u001b[0m     bnb_4bit_quant_type\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnf4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     bnb_4bit_compute_dtype\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbfloat16,\n\u001b[1;32m      5\u001b[0m     bnb_4bit_use_double_quant\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m      6\u001b[0m )\n\u001b[0;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;66;03m# silence the warnings. Please re-enable for inference!\u001b[39;00m\n\u001b[1;32m     16\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:561\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    560\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    567\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:2952\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2950\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_in_4bit \u001b[38;5;129;01mor\u001b[39;00m load_in_8bit:\n\u001b[1;32m   2951\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m quantization_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2952\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2953\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt pass `load_in_4bit`or `load_in_8bit` as a kwarg when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2954\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`quantization_config` argument at the same time.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2955\u001b[0m         )\n\u001b[1;32m   2957\u001b[0m     \u001b[38;5;66;03m# preparing BitsAndBytesConfig from kwargs\u001b[39;00m\n\u001b[1;32m   2958\u001b[0m     config_dict \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(BitsAndBytesConfig)\u001b[38;5;241m.\u001b[39mparameters}\n","\u001b[0;31mValueError\u001b[0m: You can't pass `load_in_4bit`or `load_in_8bit` as a kwarg when passing `quantization_config` argument at the same time."],"ename":"ValueError","evalue":"You can't pass `load_in_4bit`or `load_in_8bit` as a kwarg when passing `quantization_config` argument at the same time.","output_type":"error"}]},{"cell_type":"markdown","source":"- Converting a Transformers Model into a PEFT Model\n- Training with a PEFT Model And Checking Trainable Parameters of a PEFT Model","metadata":{}},{"cell_type":"code","source":"lora_model = get_peft_model(model, peft_config)\nprint(model.print_trainable_parameters())\nprint(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lora_model.print_trainable_parameters()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_arguments = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=1,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=1,\n    optim=\"paged_adamw_32bit\",\n    save_steps=25,\n    logging_steps=25,\n    learning_rate=2e-4,\n    weight_decay=0.001,\n    fp16=False,\n    bf16=False,\n    max_grad_norm=0.3,\n    max_steps=-1,\n    warmup_ratio=0.03,\n    group_by_length=True,\n    lr_scheduler_type=\"constant\",\n    report_to=\"wandb\"\n)\n\ntrainer = SFTTrainer(\n    model=lora_model,\n    train_dataset=dataset,\n    peft_config=peft_config,\n    max_seq_length= None,\n    dataset_text_field=\"text\",\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing= False,\n)\n\ntrainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Saving a Trained PEFT Model","metadata":{}},{"cell_type":"code","source":"trainer.model.save_pretrained(\"lora-google/gemma-2b\")\nwandb.finish()\nlora_model.config.use_cache = True\nlora_model.eval()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.model.push_to_hub(\"lora-google/gemma-2b\", use_temp_dir=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Performing Inference with a PEFT Model\n\nTODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning.","metadata":{}},{"cell_type":"markdown","source":"- Loading a Saved PEFT Model","metadata":{}},{"cell_type":"markdown","source":"- Generating Text from a PEFT Model","metadata":{}},{"cell_type":"code","source":"logging.set_verbosity(logging.CRITICAL)\n\nprompt = \"How do I find true love?\"\npipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\nresult = pipe(f\"<s>[INST] {prompt} [/INST]\")\nprint(result[0]['generated_text'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt = \"What is Datacamp Career track?\"\nresult = pipe(f\"<s>[INST] {prompt} [/INST]\")\nprint(result[0]['generated_text'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}
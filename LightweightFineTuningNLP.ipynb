{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Lightweight Fine-Tuning Project"]},{"cell_type":"markdown","metadata":{},"source":["TODO: In this cell, describe your choices for each of the following\n","\n","* PEFT technique: \n","* Model: \n","* Evaluation approach: \n","* Fine-tuning dataset: "]},{"cell_type":"markdown","metadata":{},"source":["## Loading and Evaluating a Foundation Model\n","\n","TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%pip -q install git+https://github.com/huggingface/transformers.git\n","%pip install -q -U git+https://github.com/huggingface/trl\n","%pip install -U accelerate\n","%pip install wandb\n","%pip install torch torchvision\n","%pip install peft\n","%pip install numpy\n","%pip install datasets\n","%pip install evaluate\n","%pip install sklearn\n","%pip install matplotlib\n","%pip install ipywidgets\n","%pip install bitsandbytes"]},{"cell_type":"code","execution_count":null,"id":"5f10453c","metadata":{"trusted":true},"outputs":[],"source":["%pip install peft\n","%pip install -U accelerate\n","%pip install -U trl \n","%pip install datasets==2.16.0"]},{"cell_type":"code","execution_count":null,"id":"8ab9d125","metadata":{"trusted":true},"outputs":[],"source":["%pip install bitsandbytes\n","!pip install GPUtil"]},{"cell_type":"code","execution_count":1,"id":"edd73081","metadata":{"execution":{"iopub.execute_input":"2024-02-29T16:20:23.728646Z","iopub.status.busy":"2024-02-29T16:20:23.727869Z","iopub.status.idle":"2024-02-29T16:20:32.481357Z","shell.execute_reply":"2024-02-29T16:20:32.480381Z","shell.execute_reply.started":"2024-02-29T16:20:23.728615Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-02-29 16:20:28.781543: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-02-29 16:20:28.781619: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-02-29 16:20:28.783447: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]}],"source":["from transformers import AutoTokenizer, TrainingArguments, Trainer, BitsAndBytesConfig, AutoModelForCausalLM, default_data_collator\n","from datasets import load_dataset\n","import numpy as np\n","from enum import Enum\n","# import evaluate\n","from peft import LoraConfig, get_peft_model, PeftModel, prepare_model_for_kbit_training\n","from torch.utils.data import Dataset, DataLoader\n","import os, wandb, torch\n","from trl import SFTTrainer"]},{"cell_type":"code","execution_count":2,"id":"a3c05dfe","metadata":{"execution":{"iopub.execute_input":"2024-02-29T16:20:36.695674Z","iopub.status.busy":"2024-02-29T16:20:36.694696Z","iopub.status.idle":"2024-02-29T16:20:36.936345Z","shell.execute_reply":"2024-02-29T16:20:36.935528Z","shell.execute_reply.started":"2024-02-29T16:20:36.695637Z"},"trusted":true},"outputs":[],"source":["from kaggle_secrets import UserSecretsClient\n","user_secrets = UserSecretsClient()\n","secret_hf = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n","secret_wandb = user_secrets.get_secret(\"wandb\")"]},{"cell_type":"code","execution_count":3,"id":"dc00bddc","metadata":{"execution":{"iopub.execute_input":"2024-02-29T16:20:39.809087Z","iopub.status.busy":"2024-02-29T16:20:39.808338Z","iopub.status.idle":"2024-02-29T16:20:42.553790Z","shell.execute_reply":"2024-02-29T16:20:42.552776Z","shell.execute_reply.started":"2024-02-29T16:20:39.809053Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n","Token is valid (permission: write).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}],"source":["!huggingface-cli login --token $secret_hf"]},{"cell_type":"code","execution_count":4,"id":"00534be3","metadata":{"execution":{"iopub.execute_input":"2024-02-29T16:20:45.268867Z","iopub.status.busy":"2024-02-29T16:20:45.267741Z","iopub.status.idle":"2024-02-29T16:21:17.987535Z","shell.execute_reply":"2024-02-29T16:21:17.986276Z","shell.execute_reply.started":"2024-02-29T16:20:45.268820Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbow1226\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"text/html":["Tracking run with wandb version 0.16.3"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20240229_162047-5wljjb2f</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/bow1226/Fine%20tuning%20mistral%207B/runs/5wljjb2f' target=\"_blank\">splendid-paper-9</a></strong> to <a href='https://wandb.ai/bow1226/Fine%20tuning%20mistral%207B' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/bow1226/Fine%20tuning%20mistral%207B' target=\"_blank\">https://wandb.ai/bow1226/Fine%20tuning%20mistral%207B</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/bow1226/Fine%20tuning%20mistral%207B/runs/5wljjb2f' target=\"_blank\">https://wandb.ai/bow1226/Fine%20tuning%20mistral%207B/runs/5wljjb2f</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["wandb.login(key = secret_wandb)\n","run = wandb.init(\n","    project='Fine tuning mistral 7B', \n","    job_type=\"training\", \n","    anonymous=\"allow\"\n",")"]},{"cell_type":"code","execution_count":14,"id":"1bdac3b1","metadata":{"execution":{"iopub.execute_input":"2024-02-29T16:25:05.102951Z","iopub.status.busy":"2024-02-29T16:25:05.102146Z","iopub.status.idle":"2024-02-29T16:25:06.398582Z","shell.execute_reply":"2024-02-29T16:25:06.396586Z","shell.execute_reply.started":"2024-02-29T16:25:05.102918Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Initial GPU Usage\n","| ID | GPU | MEM |\n","------------------\n","|  0 |  0% | 98% |\n","|  1 |  0% | 96% |\n","GPU Usage after emptying the cache\n","| ID | GPU | MEM |\n","------------------\n","|  0 | 74% |  1% |\n","|  1 | 99% |  0% |\n"]}],"source":["\n","\n","import torch\n","from GPUtil import showUtilization as gpu_usage\n","from numba import cuda\n","\n","def free_gpu_cache():\n","    print(\"Initial GPU Usage\")\n","    gpu_usage()                             \n","\n","    torch.cuda.empty_cache()\n","\n","    cuda.select_device(0)\n","    cuda.close()\n","    cuda.select_device(0)\n","\n","    print(\"GPU Usage after emptying the cache\")\n","    gpu_usage()\n","\n","free_gpu_cache()  "]},{"cell_type":"code","execution_count":12,"id":"9306cc34","metadata":{"execution":{"iopub.execute_input":"2024-02-29T16:24:34.169580Z","iopub.status.busy":"2024-02-29T16:24:34.169179Z","iopub.status.idle":"2024-02-29T16:24:34.522249Z","shell.execute_reply":"2024-02-29T16:24:34.520917Z","shell.execute_reply.started":"2024-02-29T16:24:34.169543Z"},"trusted":true},"outputs":[],"source":["import gc\n","gc.collect()\n","torch.cuda.empty_cache()"]},{"cell_type":"markdown","id":"826c42ca","metadata":{},"source":["- Prepare a dataset"]},{"cell_type":"code","execution_count":null,"id":"991e27d9","metadata":{"trusted":true},"outputs":[],"source":["base_model = \"google/gemma-2b\"\n","dataset_name = \"mlabonne/guanaco-llama2-1k\"\n","new_model = \"google/gemma-2b\""]},{"cell_type":"code","execution_count":6,"id":"42294c23","metadata":{"execution":{"iopub.execute_input":"2024-02-29T16:21:30.263016Z","iopub.status.busy":"2024-02-29T16:21:30.262665Z","iopub.status.idle":"2024-02-29T16:23:50.099283Z","shell.execute_reply":"2024-02-29T16:23:50.097148Z","shell.execute_reply.started":"2024-02-29T16:21:30.262985Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e99fa4ccd74342c9bde6880c761b355f","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"25826d08b4e84c629c533cb9665d712f","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9f6d1f21e8c54a19a00a2d75fb572a75","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4feb3aedc0724acba2fda76520d34881","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"87b3ab40086c413780a9415329a6bfeb","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c37f8a07c457461bb73dbd7af0cc7fda","version_major":2,"version_minor":0},"text/plain":["model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dfc1be29d5184ef1bfe5db4afc68009a","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c49e3a5229004beeb19405f6ae9c4f6e","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["model = AutoModelForCausalLM.from_pretrained(\n","        base_model,\n","        device_map=\"auto\",\n","        trust_remote_code=True,\n",")\n","model.config.use_cache = False # silence the warnings\n","model.config.pretraining_tp = 1\n","model.gradient_checkpointing_enable()"]},{"cell_type":"code","execution_count":7,"id":"323da16d","metadata":{"execution":{"iopub.execute_input":"2024-02-29T16:23:50.102879Z","iopub.status.busy":"2024-02-29T16:23:50.102481Z","iopub.status.idle":"2024-02-29T16:23:50.109694Z","shell.execute_reply":"2024-02-29T16:23:50.108708Z","shell.execute_reply.started":"2024-02-29T16:23:50.102838Z"},"trusted":true},"outputs":[],"source":["# tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\n","# model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\", device_map=\"auto\")"]},{"cell_type":"code","execution_count":8,"id":"2ae309fe","metadata":{"execution":{"iopub.execute_input":"2024-02-29T16:23:50.113257Z","iopub.status.busy":"2024-02-29T16:23:50.112869Z","iopub.status.idle":"2024-02-29T16:23:50.944635Z","shell.execute_reply":"2024-02-29T16:23:50.943420Z","shell.execute_reply.started":"2024-02-29T16:23:50.113217Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6bb8f7d71af94f16ba3917dd49fe5830","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/1.46k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dc63d40ac9fc40f28e056d32f28e187d","version_major":2,"version_minor":0},"text/plain":["tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d5cc40fc504140a2aa895ab72b990840","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5be1dc26a43649afb672aa69fb3787a4","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["(True, True)"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n","tokenizer.padding_side = 'right'\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.add_eos_token = True\n","tokenizer.add_bos_token, tokenizer.add_eos_token"]},{"cell_type":"code","execution_count":9,"id":"bfaa0dbf","metadata":{"execution":{"iopub.execute_input":"2024-02-29T16:23:50.946456Z","iopub.status.busy":"2024-02-29T16:23:50.946090Z","iopub.status.idle":"2024-02-29T16:23:53.303475Z","shell.execute_reply":"2024-02-29T16:23:53.302139Z","shell.execute_reply.started":"2024-02-29T16:23:50.946420Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ea6c753519954f76bc709d71cbf9c127","version_major":2,"version_minor":0},"text/plain":["Downloading readme:   0%|          | 0.00/1.02k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1365fdc9c8394a6b85c8c40b7027a81b","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/967k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8dd67b8aec2e43588eafbc9ee8ca3d74","version_major":2,"version_minor":0},"text/plain":["Generating train split:   0%|          | 0/1000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["'<s>[INST] cuanto es 2x2 xD [/INST] La respuesta es 4. </s><s>[INST] puedes demostrarme matematicamente que 2x2 es 4? [/INST] En una multiplicación, el producto es el resultado de sumar un factor tantas veces como indique el otro, es decir, si tenemos una operación v · n = x, entonces x será igual a v sumado n veces o n sumado v veces, por ejemplo, para la multiplicación 3 · 4 podemos sumar \"3 + 3 + 3 + 3\" o \"4 + 4 + 4\" y en ambos casos nos daría como resultado 12, para el caso de 2 · 2 al ser iguales los dos factores el producto sería \"2 + 2\" que es igual a 4 </s>'"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["#Importing the dataset\n","dataset = load_dataset(dataset_name, split=\"train\")\n","dataset[\"text\"][100]"]},{"cell_type":"code","execution_count":10,"id":"85607328","metadata":{"execution":{"iopub.execute_input":"2024-02-29T16:23:53.305834Z","iopub.status.busy":"2024-02-29T16:23:53.305411Z","iopub.status.idle":"2024-02-29T16:23:53.310868Z","shell.execute_reply":"2024-02-29T16:23:53.309753Z","shell.execute_reply.started":"2024-02-29T16:23:53.305793Z"},"trusted":true},"outputs":[],"source":["# torch.backends.cuda.enable_mem_efficient_sdp(False)\n","# torch.backends.cuda.enable_flash_sdp(False)"]},{"cell_type":"markdown","id":"efd6d540","metadata":{},"source":["- Create a TrainingArguments class which contains all the hyperparameters we can tune as well as flags for activating different training options then specify where to save the checkpoints from training"]},{"cell_type":"code","execution_count":15,"id":"c45df4d9","metadata":{"execution":{"iopub.execute_input":"2024-02-29T16:25:32.726276Z","iopub.status.busy":"2024-02-29T16:25:32.725378Z","iopub.status.idle":"2024-02-29T16:25:33.802418Z","shell.execute_reply":"2024-02-29T16:25:33.800951Z","shell.execute_reply.started":"2024-02-29T16:25:32.726243Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:225: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n","  warnings.warn(\n"]},{"ename":"RuntimeError","evalue":"CUDA error: invalid argument\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[15], line 31\u001b[0m\n\u001b[1;32m      1\u001b[0m training_arguments \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m      2\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./results\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     report_to\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwandb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     21\u001b[0m trainer \u001b[38;5;241m=\u001b[39m SFTTrainer(\n\u001b[1;32m     22\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     23\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mdataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     packing\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     29\u001b[0m )\n\u001b[0;32m---> 31\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:331\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trl_activate_neftune(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m--> 331\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1624\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1622\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1625\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1629\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1634\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1631\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_inner_training_loop\u001b[39m(\n\u001b[1;32m   1632\u001b[0m     \u001b[38;5;28mself\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, args\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, resume_from_checkpoint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, trial\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, ignore_keys_for_eval\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1633\u001b[0m ):\n\u001b[0;32m-> 1634\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfree_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1635\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size \u001b[38;5;241m=\u001b[39m batch_size\n\u001b[1;32m   1636\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mauto_find_batch_size:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:2971\u001b[0m, in \u001b[0;36mAccelerator.free_memory\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2969\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed_engine_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2970\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 2971\u001b[0m \u001b[43mrelease_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/utils/memory.py:61\u001b[0m, in \u001b[0;36mrelease_memory\u001b[0;34m(*objects)\u001b[0m\n\u001b[1;32m     59\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnpu\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 61\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m objects\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:159\u001b[0m, in \u001b[0;36mempty_cache\u001b[0;34m()\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Releases all unoccupied cached memory currently held by the caching\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;124;03mallocator so that those can be used in other GPU application and visible in\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;124;03m`nvidia-smi`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03m    more details about GPU memory management.\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[0;32m--> 159\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_emptyCache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: invalid argument\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}],"source":["training_arguments = TrainingArguments(\n","    output_dir=\"./results\",\n","    num_train_epochs=1,\n","    per_device_train_batch_size=4,\n","    gradient_accumulation_steps=1,\n","    optim=\"paged_adamw_32bit\",\n","    save_steps=25,\n","    logging_steps=25,\n","    learning_rate=2e-4,\n","    weight_decay=0.001,\n","    fp16=False,\n","    bf16=False,\n","    max_grad_norm=0.3,\n","    max_steps=-1,\n","    warmup_ratio=0.03,\n","    group_by_length=True,\n","    lr_scheduler_type=\"constant\",\n","    report_to=\"wandb\"\n",")\n","\n","trainer = SFTTrainer(\n","    model=model,\n","    train_dataset=dataset,\n","    max_seq_length= None,\n","    dataset_text_field=\"text\",\n","    tokenizer=tokenizer,\n","    args=training_arguments,\n","    packing= False,\n",")\n","\n","trainer.train()"]},{"cell_type":"markdown","id":"e973ba75","metadata":{},"source":["- Evaluate"]},{"cell_type":"code","execution_count":null,"id":"e55f1e14","metadata":{"execution":{"iopub.status.busy":"2024-02-29T16:24:01.429939Z","iopub.status.idle":"2024-02-29T16:24:01.430296Z","shell.execute_reply":"2024-02-29T16:24:01.430128Z","shell.execute_reply.started":"2024-02-29T16:24:01.430114Z"},"trusted":true},"outputs":[],"source":["wandb.finish()\n","model.config.use_cache = True\n","model.eval()"]},{"cell_type":"markdown","id":"9dfccbf4","metadata":{},"source":["## Performing Parameter-Efficient Fine-Tuning\n","\n","TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."]},{"cell_type":"markdown","id":"30388c72","metadata":{},"source":["- Creating a PEFT Config"]},{"cell_type":"code","execution_count":null,"id":"6c827023","metadata":{"execution":{"iopub.status.busy":"2024-02-29T16:24:01.432331Z","iopub.status.idle":"2024-02-29T16:24:01.432812Z","shell.execute_reply":"2024-02-29T16:24:01.432600Z","shell.execute_reply.started":"2024-02-29T16:24:01.432580Z"},"trusted":true},"outputs":[],"source":["model = prepare_model_for_kbit_training(model)\n","peft_config = LoraConfig(\n","    lora_alpha=16,\n","    lora_dropout=0.1,\n","    r=64,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\",\n","    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\"]\n",")\n","model = get_peft_model(model, peft_config)\n","print(model.print_trainable_parameters())\n","print(model)"]},{"cell_type":"markdown","id":"e1c0a6cb","metadata":{},"source":["- Using the bitsandbytes package to combine quantization and LoRA. This is also known as QLoRA"]},{"cell_type":"code","execution_count":null,"id":"ea95689e","metadata":{"execution":{"iopub.status.busy":"2024-02-29T16:24:01.434671Z","iopub.status.idle":"2024-02-29T16:24:01.435745Z","shell.execute_reply":"2024-02-29T16:24:01.435473Z","shell.execute_reply.started":"2024-02-29T16:24:01.435451Z"},"trusted":true},"outputs":[],"source":["bnb_config = BitsAndBytesConfig(  \n","    load_in_4bit= True,\n","    bnb_4bit_quant_type= \"nf4\",\n","    bnb_4bit_compute_dtype= torch.bfloat16,\n","    bnb_4bit_use_double_quant= False,\n",")\n","model = AutoModelForCausalLM.from_pretrained(\n","        base_model,\n","        load_in_4bit=True,\n","        quantization_config=bnb_config,\n","        torch_dtype=torch.bfloat16,\n","        device_map=\"auto\",\n","        trust_remote_code=True,\n",")\n","model.config.use_cache = False # silence the warnings. Please re-enable for inference!\n","model.config.pretraining_tp = 1\n","model.gradient_checkpointing_enable()"]},{"cell_type":"code","execution_count":null,"id":"9ea91b39","metadata":{"execution":{"iopub.status.busy":"2024-02-29T16:24:01.437147Z","iopub.status.idle":"2024-02-29T16:24:01.438241Z","shell.execute_reply":"2024-02-29T16:24:01.438004Z","shell.execute_reply.started":"2024-02-29T16:24:01.437982Z"},"trusted":true},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n","tokenizer.padding_side = 'right'\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.add_eos_token = True\n","tokenizer.add_bos_token, tokenizer.add_eos_token"]},{"cell_type":"markdown","id":"54db9c3f","metadata":{},"source":["- Converting a Transformers Model into a PEFT Model\n","- Training with a PEFT Model And Checking Trainable Parameters of a PEFT Model"]},{"cell_type":"code","execution_count":null,"id":"35896dce","metadata":{"execution":{"iopub.status.busy":"2024-02-29T16:24:01.439825Z","iopub.status.idle":"2024-02-29T16:24:01.440327Z","shell.execute_reply":"2024-02-29T16:24:01.440100Z","shell.execute_reply.started":"2024-02-29T16:24:01.440080Z"},"trusted":true},"outputs":[],"source":["lora_model = get_peft_model(model, peft_config)\n","print(model.print_trainable_parameters())\n","print(model)"]},{"cell_type":"code","execution_count":null,"id":"a080551c","metadata":{"execution":{"iopub.status.busy":"2024-02-29T16:24:01.443094Z","iopub.status.idle":"2024-02-29T16:24:01.443634Z","shell.execute_reply":"2024-02-29T16:24:01.443392Z","shell.execute_reply.started":"2024-02-29T16:24:01.443374Z"},"trusted":true},"outputs":[],"source":["lora_model.print_trainable_parameters()"]},{"cell_type":"code","execution_count":null,"id":"bcad88a5","metadata":{"execution":{"iopub.status.busy":"2024-02-29T16:24:01.444977Z","iopub.status.idle":"2024-02-29T16:24:01.445351Z","shell.execute_reply":"2024-02-29T16:24:01.445188Z","shell.execute_reply.started":"2024-02-29T16:24:01.445174Z"},"trusted":true},"outputs":[],"source":["training_arguments = TrainingArguments(\n","    output_dir=\"./results\",\n","    num_train_epochs=1,\n","    per_device_train_batch_size=4,\n","    gradient_accumulation_steps=1,\n","    optim=\"paged_adamw_32bit\",\n","    save_steps=25,\n","    logging_steps=25,\n","    learning_rate=2e-4,\n","    weight_decay=0.001,\n","    fp16=False,\n","    bf16=False,\n","    max_grad_norm=0.3,\n","    max_steps=-1,\n","    warmup_ratio=0.03,\n","    group_by_length=True,\n","    lr_scheduler_type=\"constant\",\n","    report_to=\"wandb\"\n",")\n","\n","trainer = SFTTrainer(\n","    model=lora_model,\n","    train_dataset=dataset,\n","    peft_config=peft_config,\n","    max_seq_length= None,\n","    dataset_text_field=\"text\",\n","    tokenizer=tokenizer,\n","    args=training_arguments,\n","    packing= False,\n",")\n","\n","trainer.train()"]},{"cell_type":"markdown","id":"d16a1442","metadata":{},"source":["- Saving a Trained PEFT Model"]},{"cell_type":"code","execution_count":null,"id":"96ccd4e3","metadata":{"execution":{"iopub.status.busy":"2024-02-29T16:24:01.446842Z","iopub.status.idle":"2024-02-29T16:24:01.447149Z","shell.execute_reply":"2024-02-29T16:24:01.447005Z","shell.execute_reply.started":"2024-02-29T16:24:01.446993Z"},"trusted":true},"outputs":[],"source":["trainer.model.save_pretrained(\"lora-google/gemma-2b\")\n","wandb.finish()\n","lora_model.config.use_cache = True\n","lora_model.eval()"]},{"cell_type":"code","execution_count":null,"id":"5b3be935","metadata":{"execution":{"iopub.status.busy":"2024-02-29T16:24:01.448304Z","iopub.status.idle":"2024-02-29T16:24:01.448690Z","shell.execute_reply":"2024-02-29T16:24:01.448519Z","shell.execute_reply.started":"2024-02-29T16:24:01.448498Z"},"trusted":true},"outputs":[],"source":["trainer.model.push_to_hub(\"lora-google/gemma-2b\", use_temp_dir=False)"]},{"cell_type":"markdown","metadata":{},"source":["## Performing Inference with a PEFT Model\n","\n","TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."]},{"cell_type":"markdown","metadata":{},"source":["- Loading a Saved PEFT Model"]},{"cell_type":"markdown","metadata":{},"source":["- Generating Text from a PEFT Model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-02-29T16:24:01.450292Z","iopub.status.idle":"2024-02-29T16:24:01.450775Z","shell.execute_reply":"2024-02-29T16:24:01.450551Z","shell.execute_reply.started":"2024-02-29T16:24:01.450530Z"},"trusted":true},"outputs":[],"source":["logging.set_verbosity(logging.CRITICAL)\n","\n","prompt = \"How do I find true love?\"\n","pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n","result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n","print(result[0]['generated_text'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-02-29T16:24:01.453165Z","iopub.status.idle":"2024-02-29T16:24:01.453988Z","shell.execute_reply":"2024-02-29T16:24:01.453742Z","shell.execute_reply.started":"2024-02-29T16:24:01.453720Z"},"trusted":true},"outputs":[],"source":["prompt = \"What is Datacamp Career track?\"\n","result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n","print(result[0]['generated_text'])"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30664,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":5}
